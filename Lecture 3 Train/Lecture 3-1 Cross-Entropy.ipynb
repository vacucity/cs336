{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4903db1",
   "metadata": {},
   "source": [
    "###  损失函数：交叉熵    \n",
    "交叉熵用来衡量两个概率分布之间“差异”的数学指标  \n",
    "\n",
    "- 真实分布（Target）：模型应该预测的词  \n",
    "- 预测分布（Prediction）: 模型认为词可能的分布  \n",
    "- 交叉熵：预测的分布离准确的词越远，交叉熵的值越大；反之，如果预测出了正确的词，则损失接近于0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8ca8c0",
   "metadata": {},
   "source": [
    "#### 1. 数学表示  \n",
    "对于真实分布p和预测分布q，交叉熵的公式为：  \n",
    "$$H(p, q) = - \\sum_{x \\in X} p(x) \\log q(x)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7ef816",
   "metadata": {},
   "source": [
    "在大模型任务中，正确答案通常是确定的，公式会简化为\n",
    "$$ Loss=-log(p_correct) $$\n",
    "即模型对正确的词预测的概率取对数加负号"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc2b0b7",
   "metadata": {},
   "source": [
    "### 2. 技术细节  \n",
    "大模型的训练本质是一个多分类任务。每生成一个词，模型都要从词表中选出正确的词 \n",
    "- 逐词元计算：模型在训练阶段，会根据当前给定的前一个i-1个词元，预测第i个词元出现的概率分布  \n",
    "- 损失累加：将整个序列（长度为N）中每个位置计算出的交叉熵损失取平均值，得到整个句子的总损失  \n",
    "- Softmax: 计算交叉熵之前，模型输出的是原始分数。必须经过Softmax，将这些分数转化为总和为1的概率分布 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5662ea05",
   "metadata": {},
   "source": [
    "#### 3.Z-loss与稳定性\n",
    "- 上溢（overflow）：单精度浮点数（FP32）能表示的最大值约为3.4e+38，超过这个值的数会被截断为inf，导致计算错误\n",
    "- 下溢（underflow）：FP32能表示的最小值约为1.2e-38，低于这个值的数会被截断为0，导致计算错误"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46eebbd4",
   "metadata": {},
   "source": [
    "- 对策：减去最大值M \n",
    "利用恒等式：\n",
    "$$log(\\sum_{i=1}^ne^x_i) = M + log(\\sum_{i=1}^ne^{x_i-M})$$\n",
    "\n",
    "其中M=max(0)  \n",
    "  - 减去M后，$o_j$-M的最大值为0  \n",
    "  - exp(0)=1 ，保证了求和项中至少有一个为1，彻底杜绝了分母为0的下溢风险  \n",
    "  - 所有指数都在（0，1]之间，因此对数不会出现负数，杜绝了上溢风险\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c198b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5])\n",
      "torch.Size([3])\n",
      "tensor([[1.2000, 3.1000, 0.7000, 2.4000, 0.9000],\n",
      "        [0.5000, 1.8000, 2.9000, 1.1000, 0.3000],\n",
      "        [4.2000, 2.5000, 3.7000, 1.9000, 0.8000]])\n",
      "tensor([1, 2, 0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F \n",
    "\n",
    "# shape: (batch_size, num_classes] = [3,5]\n",
    "logits = torch.tensor([\n",
    "    [1.2, 3.1, 0.7, 2.4, 0.9],  # 第1个样本\n",
    "    [0.5, 1.8, 2.9, 1.1, 0.3],  # 第2个样本\n",
    "    [4.2, 2.5, 3.7, 1.9, 0.8]   # 第3个样本\n",
    "], dtype=torch.float32)\n",
    "\n",
    "\n",
    "batch_size = logits.shape[0]\n",
    "num_classes = logits.shape[1]\n",
    "#目标类别: shape :[batch_size]=[3]\n",
    "targets = torch.tensor([1, 2, 0], dtype=torch.long)\n",
    "\n",
    "print(logits.shape)\n",
    "print(targets.shape)\n",
    "print(logits)\n",
    "print(targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24b025e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.1000, 2.9000, 4.2000])\n",
      "torch.Size([3, 5])\n",
      "tensor([[3.1000, 3.1000, 3.1000, 3.1000, 3.1000],\n",
      "        [2.9000, 2.9000, 2.9000, 2.9000, 2.9000],\n",
      "        [4.2000, 4.2000, 4.2000, 4.2000, 4.2000]])\n"
     ]
    }
   ],
   "source": [
    "#计算最大值M\n",
    "M = torch.max(logits,dim=1)[0]\n",
    "print(M)\n",
    "\n",
    "#将M扩展为和logits相同的形状，方便后续广播相减\n",
    "M_expand = M.unsqueeze(1).expand(logits.shape)\n",
    "print(M_expand.shape)\n",
    "print(M_expand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9944c7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.9000,  0.0000, -2.4000, -0.7000, -2.2000],\n",
      "        [-2.4000, -1.1000,  0.0000, -1.8000, -2.6000],\n",
      "        [ 0.0000, -1.7000, -0.5000, -2.3000, -3.4000]])\n",
      "tensor([0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "#提取目标位置的分值\n",
    "#logits - M\n",
    "logits_adjusted = logits - M_expand\n",
    "print(logits_adjusted)\n",
    "\n",
    "#提取每个样本对应目标类别的分值\n",
    "targets_scores = logits_adjusted[range(batch_size),targets]\n",
    "print(targets_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaf9141b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1496, 1.0000, 0.0907, 0.4966, 0.1108],\n",
      "        [0.0907, 0.3329, 1.0000, 0.1653, 0.0743],\n",
      "        [1.0000, 0.1827, 0.6065, 0.1003, 0.0334]])\n",
      "tensor([1.8477, 1.6632, 1.9228])\n",
      "tensor([0.6139, 0.5087, 0.6538])\n"
     ]
    }
   ],
   "source": [
    "#计算log_sum_exp\n",
    "exp_logits = torch.exp(logits_adjusted)\n",
    "print(exp_logits)\n",
    "\n",
    "#计算每行的exp_logits之和\n",
    "sum_exp_logits = torch.sum(exp_logits, dim=1)\n",
    "print(sum_exp_logits)\n",
    "\n",
    "#计算对数和\n",
    "log_sum_exp = torch.log(sum_exp_logits)\n",
    "print(log_sum_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db1aa083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6139, 0.5087, 0.6538])\n"
     ]
    }
   ],
   "source": [
    "#计算每个token的loss\n",
    "Token_loss = -targets_scores + log_sum_exp\n",
    "print(Token_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02f63d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5922)\n"
     ]
    }
   ],
   "source": [
    "#求全批次\n",
    "avg_loss = torch.mean(Token_loss)\n",
    "print(avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51be989a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5922)\n"
     ]
    }
   ],
   "source": [
    "#验证\n",
    "loss = F.cross_entropy(logits, targets)\n",
    "print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
