{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6840236",
   "metadata": {},
   "source": [
    "### Transformer Block 搭建"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94fdf6f",
   "metadata": {},
   "source": [
    "#### 1. 残差连接 \n",
    "$$ y = x + f(x) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a58032c",
   "metadata": {},
   "source": [
    "- 梯度消失：在反向传播时，残差路径是一条直达到底层的快速通道。梯度不需要再复杂的权重森林里迷路，它顺着内条先，直接回去  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadb0f65",
   "metadata": {},
   "source": [
    "- 网络退化：当网络深度增加到一定程度时，准确率会达到饱和甚至下降，这不是因为过拟合，而是深层网络难以学到“恒等变换”。残差连接通过$x+F(x)$的方式，将输入和输出进行“加法”，从而避免了网络退化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74929b31",
   "metadata": {},
   "source": [
    "#### 2. Pre-Norm和Post-Norm\n",
    "- Pre-Norm：原始transformer论文采用的方案，归一化放在残差连接之后。训练极不稳定，通常许哟复杂的“warm-up\"策略来防止训练初期崩溃。结构为：$x_n+1=Norm(x_n)+F(x)$\n",
    "- Post-Norm：归一化放在（Attention或FFN）之后，训练稳定，结构为：$x_n+1=F(Norm(x_n))$。由于归一化在残差路径之外，它保持了梯度传播的平滑，不需要复杂的预热策略也能稳定训练数百层的超深模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f93ff0",
   "metadata": {},
   "source": [
    "#### 3.Block拆分为两个部分\n",
    "##### 3.1 空间域通信（MHA）  \n",
    "空间指的为位置.一段话有10个词,就有10个空间位置.由于Transformer并行计算.\"空间域通信\"是让这些词相互连通  \n",
    "$$ x=x+self.atten(self.lnl(x),tokenz_positions=token_positions) $$\n",
    "\n",
    "- 流程:输入先经过ln1归一化,再进入atten\n",
    "- 注意点:必须将token_positions传递给self.atten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c2d0f6",
   "metadata": {},
   "source": [
    "#####  3.2 通道域提纯(FFN)\n",
    "\n",
    "$$ x=x+self.ffn(self.ln2(x)) $$\n",
    "\n",
    "- 流程:上一步的输出再次经过ln2归一化,进入ffn\n",
    "- 注意点:FFN是逐位计算的,不依赖位置信息,因此不需要传位置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7cc8e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79677cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.attn = nn.MultiheadAttention(d_model, n_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * d_model, d_model)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 注意力层 + 残差\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = residual + attn_out\n",
    "        \n",
    "        # 前馈网络 + 残差\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = residual + ffn_out\n",
    "        \n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
